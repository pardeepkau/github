{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5627b74d-3f92-404d-9f14-cdb24b36ae22",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# SQL Programming with PySpark \n",
    "SQL with PySpark Provides seamless integration with SparkSQL, allowing you to perform SQL-like queries on DataFrames. \n",
    "\n",
    "\n",
    "**Note**:\n",
    "This Notebook Applies for PySpark Env running on Local Machince"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc6cc5b4-daba-445a-8a6e-95ffa6066cc0",
     "showTitle": false,
     "title": ""
    },
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#Import Spark Packges \n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8da912c-e75a-4fe0-a5e5-1425b42ef8fc",
     "showTitle": false,
     "title": ""
    },
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#Build an Spark Session\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1906ba87-7d32-4ee4-8b43-0d728da99e93",
     "showTitle": false,
     "title": ""
    },
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Using a DataFrame Object from Spark for Employee\n",
    "empDF = spark.read.csv(\"data/emp.csv\",header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1906ba87-7d32-4ee4-8b43-0d728da99e93",
     "showTitle": false,
     "title": ""
    },
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+-----------------+----------+--------------+-------------+-------------+\n",
      "|emp_id|  emp_name|  emp_designation|emp_salary|emp_department|emp_join_date| emp_location|\n",
      "+------+----------+-----------------+----------+--------------+-------------+-------------+\n",
      "|   101|John Smith|Software Engineer|     75000|            IT|   15-01-2022|     New York|\n",
      "|   102|  Jane Doe|     Data Analyst|     60000|     Analytics|   20-08-2021|San Francisco|\n",
      "|   103|Mike Brown|  Product Manager|     90000|       Product|   10-05-2023|       London|\n",
      "|   104|Lisa Green|       HR Manager|     85000|            HR|   05-11-2020|         NULL|\n",
      "+------+----------+-----------------+----------+--------------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using a DataFrame Object from Spark for Employee\n",
    "empDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "depDF = spark.read.csv(\"data/dep.csv\",header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9a18a88-e570-4643-b0cd-d15d19b21722",
     "showTitle": false,
     "title": ""
    },
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 1. **Register DataFrame as a Temporary View:**\n",
    "Registering the DataFrame as a temporary View named 'employees'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf95f222-caad-4c83-bc92-66e9523f792f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Registering the 'empDF' DataFrame as a temporary View 'employeeView'\n",
    "empDF.createOrReplaceTempView(\"employeeView\")\n",
    "\n",
    "# Registering the 'depDF' DataFrame as a temporary View 'dedepartmentView'\n",
    "depDF.createOrReplaceTempView(\"departmentView\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7f6699b-9263-4e45-8bea-2a13e322f359",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###  2. **SQL SELECT Query:**\n",
    "Performing a SELECT query using SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgSalaryQuery = \"SELECT avg(emp_salary) as avgSalary,emp_department FROM employeeView GROUP BY emp_department\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgSalaryQueryResults = spark.sql(avgSalaryQuery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+\n",
      "|avgSalary|emp_department|\n",
      "+---------+--------------+\n",
      "|  85000.0|            HR|\n",
      "|  60000.0|     Analytics|\n",
      "|  75000.0|            IT|\n",
      "|  90000.0|       Product|\n",
      "+---------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "avgSalaryQueryResults.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68304831-5215-4860-8851-360efe6e6997",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result_df = spark.sql(\"SELECT emp_name, emp_salary FROM employeeView\")\n",
    "result_df.show()\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "045e6f65-e1f5-4fae-8133-1e259c401296",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(empDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5926ca7-429f-4f7d-920d-94411bcbc17f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 3. **AGGREATION FUCNTIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b85e7c9-30fc-4356-9396-a207ed4edff3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "avg_salaryQuery = 'SELECT AVG(emp_salary) as avg_salary,emp_department FROM employeeView GROUP BY emp_department'\n",
    "result_df = spark.sql(avg_salaryQuery)\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0454bbd-0a92-42d0-961c-cdbcec56a1f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "max_salaryQuery = 'SELECT max(emp_salary) as max_salary FROM employeeView'\n",
    "result_df = spark.sql(max_salaryQuery)\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "076416ef-4f5c-43ea-8332-49a249da128b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "min_salaryQuery = 'SELECT min(emp_salary) as min_salary FROM employeeView'\n",
    "result_df = spark.sql(min_salaryQuery)\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f836eb1f-1bc9-48a2-b746-15dfa92364d1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 4. **SQL WHERE Clause:**\n",
    "Using WHERE clause to filter rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09bf1125-465a-4c0f-989f-457b305b260e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result_df = spark.sql(\"SELECT * FROM employeeView WHERE emp_salary > 70000\")\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57b8c0df-51ad-4f8d-baca-05910f2a4fcd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 5. **SQL GROUP BY and Aggregate:**\n",
    "Using GROUP BY and AVG to calculate average salary per department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "485ffd5e-ced7-4dc2-a54a-43dbcb2697f1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result_df = spark.sql(\"SELECT emp_department, AVG(emp_salary) AS avg_salary FROM employeeView GROUP BY emp_department\")\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd11a22d-a624-4001-af8d-6278c98e68c5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 6. **SQL JOIN:**\n",
    "Creating another DataFrame for department information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e6627e2-ba24-43de-85e7-9742761235b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(empDF)\n",
    "display(depDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3353dc0f-ca67-4e13-8148-daf300a9b47b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Performing a JOIN using SQL\n",
    "result_df = spark.sql(\"SELECT e.emp_name, e.emp_salary, d.dept_name FROM employeeView e JOIN departmentView d ON e.emp_department = d.dept_name\")\n",
    "result_df.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 583110448138803,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "06_SQL_With_PySpark",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
